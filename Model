from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback
import torch

# Combine questions and answers into a single text input for the model
df['text'] = "question: " + df['question'] + " answer: " + df['summary']

# Convert the DataFrame to a Hugging Face Dataset
dataset = Dataset.from_pandas(df[['text']])

# Load the tokenizer and model
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

# Tokenize the dataset
def tokenize_function(examples):
    model_inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)  # Further reduce max_length
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)  # Further reduce max_length
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# Data collator for sequence-to-sequence tasks
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# Load the BLEU metric
bleu = load_metric("sacrebleu")

# Function to compute BLEU score
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Remove padding
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]  # sacrebleu expects a list of references

    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)
    return {
        "bleu": result["score"],
    }

# Training arguments with conditional mixed precision, further reduced batch size, and gradient accumulation
training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,  # Further reduce batch size
    gradient_accumulation_steps=8,  # Increase gradient accumulation steps
    save_steps=10_000,
    save_total_limit=2,
    fp16=use_cuda,  # Use mixed precision only if CUDA is available
    load_best_model_at_end=True,
    evaluation_strategy="steps",
    eval_steps=1000,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Train the model
trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}")

